---
title: "House sales"
author: "Erik De Luca"
date: "2023-01-15"
output:
  html_document:
    df_print: "paged"
    code_folding: hide
    toc: true
    toc_color: "orchid"
    theme: united
    keep_md: true
    toc_float: true
    number_sections: true
---

```{r,warning=FALSE, include=FALSE}
knitr::knit_engines$set(python = reticulate::eng_python)
library(leaflet)
library(dplyr)
library(MASS)
library(ggplot2)
library(RColorBrewer)
library(reticulate)
library(lime)
library(e1071)  
library(splines)
library(tidyverse)
library(data.table)
library(tseries)
library(mgcv)
library(regclass)
library(glmnet)
library(coefplot)
```

<!-- ```{js} -->
<!--   //this fails -->
<!--   $.getJSON("https://kepler.gl/demo/map?mapUrl=https://dl.dropboxusercontent.com/s/nq8hdpw5h851axg/keplergl_vh7lis1e.json", function(json) { -->
<!--       console.log(json); // this will show the info in console -->
<!--   }); -->
<!-- ``` -->

```{r}
# htmltools::includeHTML("https://kepler.gl/demo/map?mapUrl=https://dl.dropboxusercontent.com/s/nq8hdpw5h851axg/keplergl_vh7lis1e.json")
```

```{python}
#  !pip install keplergl
#  !pip install matplot
#  
#  $ jupyter nbextension install --py --sys-prefix keplergl # can be skipped for notebook 5.3 and above
# $ jupyter nbextension enable --py --sys-prefix keplergl # can be skipped for notebook 5.3 and above
 
# from keplergl import KeplerGl
# with open('data/kc_house_data_numeric.csv', 'r') as f:
#     csvData = f.read()
# map_1 = KeplerGl(height=400)
# map_1.add_data(data=csvData, name='data_2')
# map_1
# 
# import matplotlib.pyplot as plt
# plt.show()
```


# Introduzione

## Consegna

Esaminiamo i dati *kc_house_data* relativi alle transazioni immobiliari per scoprire come il prezzo sia influenzato dalla metratura delle abitazioni, considerando eventualmente anche altre caratteristiche degli immobili. 
L'obiettivo di questa analisi è quindi quello di individuare eventuali connessioni tra le diverse variabili tramite la creazione di un modello di regressione.

Nel grafico (immagine seguente) sono presenti degli esagoni con una scala di colori dal blu al rosso, se la media delle case all'interno dell'esagono è bassa sarà più tendente al blu, viceversa, di colore rosso.
L'altezza delle barre rappresenta la densità di case nell'area coperta dall'esagono. 
La mappa interattiva è disponibile [cliccando qui](https://kepler.gl/demo/map?mapUrl=https://dl.dropboxusercontent.com/s/nq8hdpw5h851axg/keplergl_vh7lis1e.json).

```{r}
knitr::include_graphics("image/kepler.png")
```


## Importazione dati

Per cominciare, dobbiamo importare i dati $kc_house_data$ e rappresentarli in modo appropriato. Utilizzando i pacchetti giusti e le funzioni adeguate, possiamo caricare i dati in un dataframe e visualizzarli in modo da avere una prima idea delle informazioni a disposizione e di come siano organizzati. Successivamente, possiamo utilizzare diverse tecniche di rappresentazione dei dati, come grafici e tabelle, per esplorare ulteriormente i dati e comprendere meglio la relazione tra prezzo e metratura delle abitazioni.


```{r}
# dati presi da kaggle
df = "data/kc_house_data.csv" %>% 
  read.csv() %>% 
  tibble()

# creo nuovi file csv per la mappa Kepler.gl
"data/kc_house_data.csv" %>% 
  read.csv() %>% 
  tibble() %>% 
  mutate_at("price", as.numeric) %>% 
  write.csv(file = "data/kc_house_data_numeric.csv", quote = T)

"data/kc_house_data.csv" %>% 
  read.csv() %>% 
  tibble() %>% 
  mutate_at("price", as.numeric) %>% 
  slice(which(price > quantile(df$price,.999) %>% as.numeric)) %>% 
  write.csv(file = "data/lusso999.csv", quote = T)

df
```

```{r}
pal = with(df, colorFactor(brewer.pal(10,"RdYlGn"), price))
dfPopup = df %>% 
  mutate(popup_info = paste("Prezzo della casa: ", price, " $", "</br>",
                            "Superficie del soggiorno: ", sqft_living, "</br>", 
                            "Superficie del seminterrato: ", sqft_basement, "</br>", 
                            "Superficie del lotto: ", sqft_lot, "</br>",
                            "Numero di piani: ", floors, "</br>", 
                            "Numero di bagni: ", bathrooms, "</br>",
                            "Numero di camere: ", bedrooms, "</br>",
                            "Vista sul mare: ", waterfront, "</br>",
                            "Data di vendita dell'immobile: ", date, "</br>",
                            "Numero di visite: ", view, "</br>", 
                            "Valutazione: ", grade, " ", condition, "</br>",
                            "Coordinate: ", lat, " ", long, "</br>",
                            "Anno di costruzione: ", yr_built, "</br>", 
                            "Anno del restauro: ", yr_renovated, "</br>"))
leaflet() %>% 
  addTiles() %>% 
  addCircleMarkers(data = dfPopup,
                   lat = ~ lat,
                   lng = ~ long,
                   radius = ~ 1,
                   color = ~ pal(price),
                   popup = ~ popup_info)

```


## Descrizione dataset

Il dataset che ci troveremo ad analizzare è composto da 21597 esempi di case vendute nella contea di King, nello stato di Washington (USA), tra Maggio 2014 e Maggio 2015. 
Il dataset include informazioni su 21 variabili differenti, come prezzo, metratura, numero di camere da letto, anno di costruzione, e così via. Si tenga presente che questi dati coprono solo parzialmente la città di Seattle. 
L'obiettivo di questa analisi sarà quello di esplorare le relazioni tra le variabili presenti nel dataset e capire come influiscano sul prezzo delle case, utilizzando una vasta gamma di tecniche statistiche e di visualizzazione.

Breve descrizione delle 21 variabili (Data type):

- `id`: numero identificativo associato all'i-esima casa venduta (Numeric);

- `date`: la data in cui la casa i-esima è stata venduta (String);

- `price`: la variabile risposta (Numeric);

- `bedrooms`: n° di camere da letto per casa (Numeric);

- `bathrooms`: n° di bagni per camera (Numeric);

- `sqftliving`: n° di piedi quadrati del soggiorno (Numeric);

- `sqftlot`: n° di piedi quadrati del lotto (Numeric);

- `floors`: n° di piani (Numeric);

- `waterfront`: se la casa ha vista sul mare/fiume/lago (Numeric);

- `view`: n° di visite effettuate (Numeric);

- `condition`: valutazione complessiva delle condizioni della casa, dove 1 indica usurata e 5 indica eccellente. Per maggiori informazioni: (http://info.kingcounty.gov/assessor/esales/Glossary.aspx?type=r#g) (Numeric);

- `grade`: grado complessivo assegnato all'abitazione, basato sul King County grading system: 1 povera, 13 eccellent. (Numeric);

- `sqftabove`: piedi quadri della casa escluso il seminterrato (Numeric);

- `sqftbasement`: piedri quadri del seminterrato (Numeric);

- `yrbuilt`: anno di costruzione (Numeric);

- `yrrenovated`: anno di restauro della casa (Numeric);

- `zipcode`: codice zip (Numeric);

- `lat`: Latitude coordinate (Numeric);

- `long`: Longitude coordinate (Numeric);

- `sqftliving15`: zona giorno nel 2015(dato implicato in caso di restauri dell'area del lotto), si noti che questo dato potrebbe non aver avuto effetto sui piedri quadri del lotto (Numeric);

- `sqftlot15`: piedri quadri del lotto nel 2015 (dato implicato in caso di restauri dell'area del lotto) (Numeric).

La presenza della variabile `grade` nel dataset, basata su un sistema di classificazione specifico per la contea di King, suggerisce che i dati potrebbero essere stati raccolti da una fonte ufficiale come ad esempio l'ufficio delle tasse del contea.
Inoltre, la presenza della variabile `view`, che indica il numero di visite effettuate ad una casa, suggerisce che i dati potrebbero provenire da un'agenzia immobiliare o un'altra fonte simile.
Anche se non ci sono ragioni per mettere in discussione l'accuratezza generale dei dati, è sempre opportuno considerare con cautela l'applicabilità dei modelli basati su questo dataset a casi più generali, tenendo in considerazione la possibile limitatezza geografica e temporale dei dati e l'eventuale influenza di variabili non incluse nel dataset.

# Analisi preliminare dei dati

## Variabile risposta: Price

Per la scelta della variabile risposta il percorso è imposto, va utilizzata la variabile `price`, ma vanno considerati i seguenti accorgimenti per l'assunzione sul modello parametrico da utilizzare:
- Per la costruzione di un modello di regressione lineare, condizione necessaria è che la variabile risposta sia una variabile quantitativa continua con distribuzione normale;
- Per un modello glm gamma la variabile risposta deve essere quantitativa continua con supporto in R+ e deve assumere la forma funzionale per il glm gamma.

Per arrivare ai seguenti risultati ci arriveremo tramite delle trasformazioni.

```{r, warning=FALSE}
ggplot(df, aes(x = price, y = after_stat(density))) +
  geom_histogram(color = "orchid",
               fill = "orchid",
               alpha = 0.5,
               bins = 100) + 
  geom_density(color = "paleturquoise",
               fill = "paleturquoise",
               alpha = 0.3, # densità del colore 
               kernel = "gaussian",
               adjust = 1) +
  geom_line(aes(y = dnorm(price, mean(price), sd(price))),
            color = "aquamarine") +
  geom_line(aes(y = dgamma(price,
                           fitdistrplus::fitdist(df$price, "gamma", method = "mme")$estimate[1],
                           fitdistrplus::fitdist(df$price, "gamma", method = "mme")$estimate[2])),
            color = "blueviolet") +
  xlim(c(0,3E6)) 
```

### Traformazione della variabili risposta

I dati della variabile `price` non si distribuiscono perfettamente:

```{r}
shapiro.test(df$price[sample(1:nrow(df), 5000)])
jarque.bera.test(df$price)
tibble(kurtosis = kurtosis(df$price), skewness = skewness(df$price))
```

Nei due grafici seguenti si può osservare la distribuzione dei quantili teorici di una distribuzione normale con i quantili teorici osservati. I quantili empirici del logaritmo di `price` si distribuiscono maggiormente sulla retta che divide i quadranti del grafico.
Si può constatare che, nonostante la trasformazione logaritmica, la distribuzione continua a possedere un'assimetria con una coda di destra più pesante.

```{r}
df %>% 
  dplyr::select(price) %>% 
  add_column("type" = "price") %>% 
  add_row(price = df$price %>% log, type = "log(price)") %>% 
  ggplot(aes(sample = price)) +
  geom_qq(color = "orchid",
          alpha = 0.8) +
  geom_qq_line(color = "aquamarine",
          alpha = .8,
          linewidth = 1) +
  facet_wrap(vars(type), 
             scale = "free_y")
```

Provo a effetuare una trasformazione logaritmica per provare a vedere se con questa trasformazione i miei dati si distribuiscono meglio secondo una normale o una distribuzione gamma.

```{r, warning=FALSE}
df %>% 
  mutate_at("price",log) %>% 
  ggplot(aes(x = price, y = after_stat(density))) +
  geom_histogram(color = "orchid",
               fill = "orchid",
               alpha = 0.5,
               bins = 100) + 
  geom_density(color = "paleturquoise",
               fill = "paleturquoise",
               alpha = 0.3,
               kernel = "gaussian",
               adjust = 1) +
  geom_line(aes(y = dnorm(price, mean(price), sd(price))),
            color = "aquamarine") +
  geom_line(aes(y = dgamma(price,
                           fitdistrplus::fitdist(df$price %>% log, "gamma", method = "mme")$estimate[1],
                           fitdistrplus::fitdist(df$price %>% log, "gamma", method = "mme")$estimate[2])),
            color = "blueviolet")
```

  Mentre prima la distribuzione gamma si adattava molto meglio ai dati della normale, ora le due distribuzioni appaiono molto simili.

```{r}
df = df %>% mutate("lprice" = log(price))
paste("Skewness:", skewness(df$lprice),
      "Kurtosis: ", kurtosis(df$lprice))
shapiro.test(df$price[sample(1:nrow(df), 5000)])
shapiro.test(df$lprice[sample(1:nrow(df), 5000)])
```

## Altre variabili

### Zipcode

Osservando come sono formati i dati si nota che `zipcode` è considerata come una variabile numerica quando, invece, una sua classificazione adeguata sarebbe come variabile qualitativa nominale (non ordinale).
Inoltre, si pensa che la variabile possa molto influire nel prezzo, in quanto la zona della città influenza di molto il prezzo delle abitazioni che vi sono dentro.

```{r}
df = df %>% mutate_at("zipcode", as.factor)
df$zipcode = reorder(df$zipcode,df$lprice)

ggplot(df, aes(zipcode, lprice)) +
  geom_boxplot(outlier.colour = "orchid",
               outlier.alpha = 0.7,
               outlier.size = 0.7) +
  theme(axis.text.x=element_text(vjust = 1,size = 8, angle = 90))
```

Il grafico geospaziale seguente è utile per comprendere meglio come si distribuiscano le zone della città e per capire eventuali patterns che ci sono tra zone contigue.

```{r}
# con brewer.pal creo una palette di 10 colori dal verde alrosso
# con colorRampPalette amplio i colori da 10 al numero di zipcode presenti
# con colorFactor assegno ogni colore al valore della media dei prezzi delle case nel zipcode pres in considerazione 
pal = with(df,colorFactor(colorRampPalette(brewer.pal(10,"RdYlGn"))(df$zipcode %>% levels() %>% length()),as.data.frame(tapply(df$price,df$zipcode,mean))[zipcode,]))
                        
leaflet() %>% 
  addTiles() %>% 
  addCircleMarkers(data = df,
                   lat = ~ lat,
                   lng = ~ long,
                   radius = ~ 1,
                   color = ~ pal(as.data.frame(tapply(df$price,df$zipcode,mean))[zipcode,]))
```

<!-- #### Anno di costruzione e di ristrutturazione -->

<!-- ```{r} -->

<!-- #  -->
<!-- # ggplot(df,aes(yr_built,lprice)) +geom_point() + geom_smooth() -->
<!-- # ggplot(df,aes(yrRenBuilt,lprice)) +geom_point() + geom_smooth(formula = y ~ ns(x,knots = 2)) -->

<!-- ``` -->


### Grade

Provo a stimare un GAM tramite una spline naturale con 5 nodi. 

```{r, warning=FALSE}
ggplot(df,aes(grade,lprice)) + 
  geom_jitter(width = .2,
              color = "gray25",
              size = 0.7) +
  geom_smooth(method = 'gam',
              formula = y ~ ns(x, knots = 5),
              color = "orchid",
              fill = "orchid1",
              alpha = 0.3)
```

### Bathrooms

Anche in questo caso viene stimata una spline naturale in modo da garantire buone stime sui valori estremi, in quanto la funzione non assumerà un comportamento polinomiale.
Una valida alternativa poteva essere usare una B-spline.

```{r}
ggplot(df,aes(bathrooms,lprice)) + 
  geom_jitter(width = .2,
              color = "gray25",
              size = 0.7) +
  geom_smooth(method = 'gam',
              formula = y ~ ns(x, knots = 5),
              color = "orchid",
              fill = "orchid1",
              alpha = 0.3)
```

### Sqft_basement

Interessante notare come la variabile `sqft_basement` contenga tantissimi valori pari a zero che indicano una mancanza d'informazione.
Quindi, nel caso sia molto correlata con altre variabili con meno o informazioni mancanti, converrà usare queste ultime. 
Le procedure appena illustrate andrebbero fatte con tutte le altre variabili indipendenti possibili.

```{r}
ggplot(df,aes(sqft_basement,log(price))) + 
  geom_jitter(width = .2,
              color = "gray25",
              size = 0.7) +
  geom_smooth(method = 'gam',
              formula = y ~ ns(x, knots = 25),
              color = "orchid",
              fill = "orchid1",
              alpha = 0.3)
```

## Collinearità

Bisogna evitare la multicollinearità nel modello.
Per far ciò si va inanzitutto a visualizzare un correlogramma, ovvero un grafico che mostra le correlazioni tra le variabili.

Per evitare la collinearità si potrà fare un analisi delle componenti principali (PCA), evitare alcune variabili, o applicare una penalizzazione.

```{r}
df %>% 
  select_if(is.numeric) %>% 
  dplyr::select(-c(id, lat, long, lprice)) %>% 
  cor %>% 
  corrplot::corrplot(method = "number",
                     hclust.method = "ward.D2",
                     diag = F,
                     type = "upper",
                     order = "hclust",
                     number.cex = .6)
```

# Modello

## Modello iniziale

Analizzando la correlazione tra variabili e osservando come le variabili si distribuiscano al variare della variabile risposta (`lprice`), si opteranno per delle determinate scelte:
- `sqft_living` come una B-spline con 30 nodi
- `zipcode` in quanto si è visto che come variabile categorica ha un'influenza decisa sulla variabile risposta
- viene inserita anche l'interazione tra `grade` e `sqft_living` a causa della loro forte correlazione.

Già con questo modello iniziale si ottiene una devianza spiegata del 87.1%.



```{r}
parametri = list(formula = lprice ~ zipcode + bs(sqft_living, knots = 30)*grade + bs(sqft_basement, knots = 30) + view + waterfront,
            data = df)
modIniziale = do.call(gam, append(parametri, list(family = gaussian(link = identity), method = "ML")))
modIniziale %>% summary
```

L'interazione tra `sqft_living` e `grade` causa ha una S.E. molto elevato, così anche le rispettive variabili prese da sole. 
Provo a rimuovere l'interazione per vedere se le stime dei coefficienti migliorano.


```{r}
parametri = list(formula = lprice ~ zipcode + bs(sqft_living, knots = 30) + bs(sqft_basement, knots = 30) + grade + view + waterfront,
            data = df)
modIniziale = do.call(gam, append(parametri, list(family = gaussian(link = identity), method = "ML")))
modIniziale %>% summary
```

## Famiglie parametriche

A scopo illustrativo vengono creati altri modelli con altre distribuzioni e di conseguenza altre famiglie distributive e funzioni legame.
All'interno della tabella seguente ci sono alcuni parametri e test dei diversi modelli.

Alcune distribuzioni non potevano essere inserite, come la binomiale o la poisson. 
Ad esempio, considerando la poisson, essa è una distribuzione di probabilità discreta a differenza del logaritmo del prezzo delle case. 
Inoltre, essa esprime le probabilità per il numero di eventi che si verificano successivamente, quindi non compatibile con i nostri dati. 
Tra i modelli è stato inserito anche quello di quasipoisson che è diverso dalla distribuzione di poisson.
Sul quasipoisson ci sono solo le assunzioni del secondo ordine e quindi non c'è l'ipotesi distributiva.
Quindi, avrà in comune con la distribuzione di poisson la funzione legame. 
Ciò viene fatto solo a scopo illustrativo, in quanto è meglio utilizzare, se possibile, un'ipotesi distributiva che ha proprietà più forti; nel nostro caso la gamma sostituisce molto meglio la quasipoisson. 

Partendo dalla prima colonna si nota che la quasipoisson non possiede AIC, questo è dovuto al fatto che l'AIC si calcola a partire dalla funzione di verosimiglianze.
La quasipoisson non possiede una funzione di verosimiglianza in quanto non ha un'ipotesi distributiva, possiede una funzione di quasiverosimiglianza.

Per le distribuzioni Gamma e quasipoisson è stato stimato il fattore di scala, $\psi$. Nella quasipoisson $\psi$ è stimato.
  
La devianza, così come il criterio di validazione incrociata generalizzata (GCV), risulta nettamente minore per il modello costruito con la distribuzione gamma.

```{r}
famiglie = c("quasipoisson", "Gamma", "gaussian")
modelli = lapply(1:length(famiglie), function(i) do.call(gam, append(parametri, list(family =  get(famiglie[i])))))
tibble(famiglie,
       aic = sapply(1:length(famiglie), function(i) modelli[[i]]$aic),
       scale = sapply(1:length(famiglie), function(i) modelli[[i]]$scale),
       sigma2 = sapply(1:length(famiglie), function(i) modelli[[i]]$sig2),
       deviance = sapply(1:length(famiglie), function(i) modelli[[i]]$deviance),
       gcv.ubre = sapply(1:length(famiglie), function(i) modelli[[i]]$gcv.ubre),
       )
```


# Modelli annidati

## Modello pieno

Modello con tutte le variabili d'interesse.

```{r}
modPieno = glm(formula = paste("lprice ~",paste(df %>% dplyr::select(-lprice, -id, -lat, -long, -date, -price) %>% names, collapse = "+")) %>% as.formula,
               data = df,
               family = gaussian)
plot(modPieno)
```

## Metodi iterativi

```{r}
modAIC = stepAIC(modPieno, 
                 steps = 100,
                 direction = "backward")
```


# Qualità del modello

Il grafico del qq-plot ci mostra che l'assunzione di normalità dei residui non è rispettata, notiamo la presenza di code pesanti.

```{r}
gam.check(modIniziale)
```


## Analisi dei residui

```{r}
# standardizzo i residui e faccio un grafico
with(modIniziale,(residuals - mean(residuals))/sd(residuals)) %>% 
  as.data.frame() %>% 
  ggplot(aes(x = ., y = after_stat(density))) + 
  geom_histogram(color = "orchid",
               fill = "orchid",
               alpha = 0.5,
               bins = 40) + 
  geom_density(color = "paleturquoise",
               fill = "paleturquoise",
               alpha = 0.3, # densità del colore 
               kernel = "gaussian",
               adjust = 1) +
  geom_line(aes(y = dnorm(., 0, 1)),
            color = "aquamarine")
```

```{r, warning=FALSE}
ks.test(with(modIniziale,(residuals - mean(residuals))/sd(residuals)),
        rnorm(length(df$price),0,1))
```

# Penalizzazione

Interessante vedere come tramite penalizzazione la stima del coefficiente della variabile indipendente `sqft_living`, che era la variabile maggiormente correlata con la variabile dipendente, questo dovuto al fatto che essa era fortemente correlata con altre variabili indipendenti.

```{r}
modLasso = glmnet(makeX(df[, !names(df) %in% c("lprice","id","lat","long", "price", "date")]),
                       df$lprice,
                       alpha = 1)
coefplot(modLasso,
         intercept = F,
         interactive = T,cex = 0.5,
         coefficients = names(df)[!names(df) %in% c("lprice","id","lat","long", "price", "date")])
```

```{r}
coefplot(modLasso,
         intercept = F,
         interactive = T,
         cex = 0.5,
         coefficients = names(modIniziale$coefficients)[names(modIniziale$coefficients) %like% "zipcode"])
```


```{r}
plot(modLasso)
```

